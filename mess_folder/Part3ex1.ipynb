{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.openalex.org/authors\"\n",
    "email = \"s204120@dtu.dk\"\n",
    "\n",
    "#Only used for Week 2 name searching so code is relatively slow on purpose to avoid API errors\n",
    "def get_author_data(name):\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params={'search': name, 'mailto': email}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'results' in data and data['results']:\n",
    "            author = data['results'][0]  # Take the first result\n",
    "            institutions = author.get('last_known_institutions', [])\n",
    "            country_code = institutions[0].get('country_code', 'N/A') if institutions else 'N/A'\n",
    "\n",
    "            return {\n",
    "                'id': author.get('id', 'N/A'),\n",
    "                'display_name': author.get('display_name', 'N/A'),\n",
    "                'works_api_url': author.get('works_api_url', 'N/A'),\n",
    "                'h_index': author.get('summary_stats', {}).get('h_index', 0),\n",
    "                'works_count': author.get('works_count', 0),\n",
    "                'country_code': country_code\n",
    "            }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {name}: {e}\")\n",
    "    \n",
    "    return None  # Return None if an error occurs\n",
    "#10 Authors and parallel jobs (theoretical max: 25 author, 10 jobs /s)\n",
    "def process_in_batches(author_names, batch_size=10, delay=1.0):\n",
    "    authors_data = []\n",
    "    for i in tqdm(range(0, len(author_names), batch_size), desc=\"Fetching author data\"):\n",
    "        batch = author_names[i:i + batch_size]  # Take a batch of names\n",
    "        results = Parallel(n_jobs=batch_size)(delayed(get_author_data)(name) for name in batch)\n",
    "        authors_data.extend([author for author in results if author])\n",
    "        time.sleep(delay)  # enforcing 10 API calls /s max\n",
    "\n",
    "    return authors_data\n",
    "    \n",
    "#Generate API info on author_names_2024:\n",
    "authors_data = process_in_batches(author_names_2024, batch_size=10, delay=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the output and filtering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(authors_data)\n",
    "#Remove rows with works outside of range [5,5000]\n",
    "df_filtered = df[(df['works_count'] >= 5) & (df['works_count'] <= 5000)]\n",
    "df_filtered.to_csv('author_names_plain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of author ID's for use in the IC2S2 datasets, API calls:\n",
    "IC2S2_author_IDs = [ids[-11:] for ids in pd.read_csv(\"author_names_plain.csv\").id.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching author works from author ID's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Concepts\n",
    "# can find all concept ID's at: https://api.openalex.org/concepts\n",
    "concepts1 = ['C144024400', 'C15744967', 'C162324750', 'C17744445']\n",
    "concepts2 = ['C33923547', 'C121332964', 'C41008148']\n",
    "EMAIL = \"s204120@dtu.dk\"\n",
    "\n",
    "# Function to generate OpenAlex API request URL\n",
    "def URL_filter(author_ids):\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    author_filter = f\"author.id:{'|'.join(author_ids)}\"\n",
    "    concepts1_filter = '|'.join(concepts1)\n",
    "    concepts2_filter = '|'.join(concepts2)\n",
    "    filters = (\n",
    "        \"?filter=\"\n",
    "        f\"{author_filter},\"\n",
    "        f\"cited_by_count:>10,\"\n",
    "        f\"concepts.id:({concepts1_filter}),\"\n",
    "        f\"concepts.id:({concepts2_filter})\"\n",
    "    )\n",
    "    \n",
    "    return base_url + filters\n",
    "\n",
    "# Function to fetch works for a given batch of authors\n",
    "def fetch_author_works(author_ids, max_results=200, cursor=\"*\"):\n",
    "    all_works = []\n",
    "    \n",
    "    while cursor:\n",
    "        url = URL_filter(author_ids)\n",
    "        params = {\n",
    "            'per_page': max_results, \n",
    "            'cursor': cursor,  \n",
    "            'mailto': EMAIL,\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if 'results' in data and data['results']:\n",
    "            all_works.extend(data['results'])\n",
    "        cursor = data.get('meta', {}).get('next_cursor', None)\n",
    "\n",
    "    return all_works\n",
    "\n",
    "# Function to extract relevant details from works\n",
    "def extract_work_details(works):\n",
    "    works_data = []\n",
    "    for work in works:\n",
    "        work_info = {\n",
    "            'id': work.get('id', 'N/A'),\n",
    "            'publication_year': work.get('publication_year', 'N/A'),\n",
    "            'cited_by_count': work.get('cited_by_count', 0),\n",
    "            'author_ids': [author['author'].get('id', 'N/A') for author in work.get('authorships', [])],\n",
    "            'title': work.get('title', 'N/A'),\n",
    "            'abstract_inverted_index': work.get('abstract_inverted_index', 'N/A'),\n",
    "        }\n",
    "        works_data.append(work_info)\n",
    "    return works_data\n",
    "\n",
    "\n",
    "def fetch_and_process_batch(chunk):\n",
    "    works = fetch_author_works(chunk)\n",
    "    return extract_work_details(works)\n",
    "\n",
    "#Batch size = how many authors per API request, n_jobs are parallel threads.\n",
    "def Request_batching(author_ids, batch_size=25, n_jobs=9):\n",
    "    chunks = [author_ids[i:i + batch_size] for i in range(0, len(author_ids), batch_size)]\n",
    "    \n",
    "    all_works_data = []\n",
    "    with tqdm(total=len(chunks), desc=\"Fetching batches\") as pbar:\n",
    "        for i in range(0, len(chunks), n_jobs):  # Process in groups of 10\n",
    "            batch_chunks = chunks[i:i + n_jobs]  # Take up to 10 batches\n",
    "            results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(fetch_and_process_batch)(chunk) for chunk in batch_chunks\n",
    "            )\n",
    "            \n",
    "            for batch in results:\n",
    "                all_works_data.extend(batch)\n",
    "            \n",
    "            pbar.update(len(batch_chunks))  \n",
    "            time.sleep(1)  \n",
    "\n",
    "    return all_works_data\n",
    "    \n",
    "#Parent function to call the API using the ID list:\n",
    "all_works_data = Request_batching(IC2S2_author_IDs)\n",
    "\n",
    "df = pd.DataFrame(all_works_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating the into the two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers = pd.DataFrame(all_works_data, columns=['id', 'publication_year', 'cited_by_count', 'author_ids'])\n",
    "df_abstracts = pd.DataFrame(all_works_data, columns=['id', 'title', 'abstract_inverted_index'])\n",
    "df_papers.to_csv(\"IC2S2_Papers.csv\", index=False)\n",
    "df_abstracts.to_csv(\"IC2S2_Abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset constitutes: 13806 works, 21160 unique authors.\n",
    "\n",
    "In order to speed up the code several techniques were employed. We made sure to have the maximum allowed 25 authors in each API call by using the '|' notation in the URL. This was achieved by creating a batcher function that split the list of author ID's. \n",
    "\n",
    "We used the joblib to parralelize the API, using 9 calls at a time with 25 authors each. We erred on the side of caution here to avoid API errors, as well as making sure to implement a one second sleep timer between calls to avoid rate limiting. \n",
    "\n",
    "Another efficiency increase comes from including the concept filtering and inequalities directly in the filter with the API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coarse concept definitions gives us a large range of potential works, that's still focussed. This is probably the variable that would change the potential scope the most.\n",
    "\n",
    "The work range makes sure we don't get authors with very little input, or ones where SoSci themes aren't their main research subject. In other words, giving us more relevant authors. The 5000 cap makes sure we don't balloon the dataset if there are anomalies in the API with extreme amounts of attatched works. \n",
    "\n",
    "Since the concepts are so coarse though, we could risk getting a dataset that is very broad in nature. Concepts such as math and computer science will most likely be present in a very large majority of technical works. \n",
    "\n",
    "Authors per work will also likely focus the dataset away from works with very broad scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
